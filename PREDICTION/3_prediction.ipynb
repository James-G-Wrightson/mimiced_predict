{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a6cee3",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc965e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Modelling\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "from sklearn.metrics import average_precision_score, classification_report,  roc_auc_score, accuracy_score, confusion_matrix\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "## SHAP plot\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Admin\n",
    "import pickle\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a6f29",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "28d88c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.read_parquet('../MIMICED/ed_join_cc.parquet')\n",
    "cc = pd.read_parquet('../MIMICED/chiefcomp_clustered.parquet')\n",
    "cc = cc.drop(columns=['chiefcomplaint','cc_cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971f335e",
   "metadata": {},
   "source": [
    "Join the files by stay_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "de052cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = data_all.merge(cc, how='left', on='stay_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc316e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb94f4f",
   "metadata": {},
   "source": [
    "## Wrangle\n",
    "\n",
    "Select columns for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c1763dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_keep = [ \n",
    "    'gender', \n",
    "    'age',\n",
    "    'race',\n",
    "    'arrival_transport',\n",
    "    'pain',\n",
    "    'temperature',\n",
    "    'heartrate',\n",
    "    'resprate',\n",
    "    'o2sat',\n",
    "    'sbp',\n",
    "    'dbp',\n",
    "    'subject_id',\n",
    "    'intime',\n",
    "    'icd_chapter',\n",
    "    'revisit_72hrs',\n",
    "    'med_gsn_count',\n",
    "    'prior_visits_1yr',\n",
    "    'acuity',\n",
    "    'cci_score', \n",
    "    'cluster', \n",
    "    'res',\n",
    "    'nyu', \n",
    "    'disposition'\n",
    "    ]\n",
    "data = data_all[cols_keep].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f2d26",
   "metadata": {},
   "source": [
    "Make the CCI into as category with fewer levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aa5c1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['CCI_cat'] = np.where(data['cci_score'] >= 5, '6', data['cci_score'].astype(str))\n",
    "data = data.drop(columns=['cci_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc619de",
   "metadata": {},
   "source": [
    "Recode the number of visits in the last 12 months into a category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6247bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['previous_cat'] = np.where(data['prior_visits_1yr'] >= 1, 1, 0)\n",
    "data = data.drop(columns=['prior_visits_1yr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7a19e5",
   "metadata": {},
   "source": [
    "Get the previous diagnosis (as icd chapter) if there was one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c3022a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(by=['subject_id', 'intime'])\n",
    "data['icd_chapter_lag'] = data.groupby('subject_id')['icd_chapter'].shift(1)\n",
    "data['previous_result'] = np.where(\n",
    "    data['previous_cat'] == 1,\n",
    "    data['icd_chapter_lag'],\n",
    "    'none'\n",
    ")\n",
    "data = data.drop(columns=['icd_chapter_lag', 'icd_chapter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16a86f",
   "metadata": {},
   "source": [
    "Create a time category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "81b116a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_time(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return 'morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'evening'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "data['time_of_day'] = data['intime'].dt.hour.apply(categorize_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb6797e",
   "metadata": {},
   "source": [
    "Create the outcome admitted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1ebf7c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['home'] = data['disposition'].apply(lambda x: 1 if x == 'HOME' else 0)\n",
    "data = data.drop(columns=['disposition'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c00ca7",
   "metadata": {},
   "source": [
    "Ensure categpries are categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ad27ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['nyu'] = data['nyu'].astype('int64')\n",
    "data['res'] = data['res'].astype('int64')\n",
    "\n",
    "columns_cat = ['gender', 'race','arrival_transport', 'revisit_72hrs','CCI_cat','previous_cat', 'cluster','time_of_day', 'nyu', 'res' , 'home', 'previous_result']    \n",
    "for col in columns_cat:      \n",
    "    data[col] = data[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce18112",
   "metadata": {},
   "source": [
    "For meds count, NA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d07d3c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['med_gsn_count'] = data['med_gsn_count'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef330d9",
   "metadata": {},
   "source": [
    "Group the race variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "41cecebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    data['race'].str.contains('WHITE', case=False, na=False),\n",
    "    data['race'].str.contains('BLACK', case=False, na=False),\n",
    "    data['race'].str.contains('HISPANIC', case=False, na=False),\n",
    "    data['race'].str.contains('ASIAN', case=False, na=False)\n",
    "]\n",
    "choices = ['White', 'Black', 'Hispanic/Latino', 'Asian']\n",
    "data['race'] = np.select(conditions, choices, default='Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773fb03",
   "metadata": {},
   "source": [
    "Keep only the last visit (i.e. one per subject, but preserving info about previous visits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "52c25c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_visit_idx = data.groupby('subject_id')['intime'].idxmax()\n",
    "data = data.loc[last_visit_idx].reset_index(drop=True)\n",
    "data = data.drop(columns=['intime', 'subject_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb84a0",
   "metadata": {},
   "source": [
    "## Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5fe147da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot variable distributions\n",
    "def plot_function(df, outcome):\n",
    "    # Suppress warnings about ticks\n",
    "    warnings.filterwarnings('ignore')\n",
    "    # var types\n",
    "    cont_vars  = df.drop(columns = [outcome]).select_dtypes(include=['int','float']).columns.tolist()\n",
    "    cat_vars = df.drop(columns =[outcome]).select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "\n",
    "    # plot dim\n",
    "    num_plots = len(cont_vars) + len(cat_vars) \n",
    "    cols = 3\n",
    "    rows = int(np.ceil(num_plots / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 4))\n",
    "    axes = axes.flatten()\n",
    "    plot_idx = 0\n",
    "\n",
    "    # plot continuous\n",
    "    for col in cont_vars:\n",
    "        sns.kdeplot(data=df, x=col, hue=outcome, fill=True, common_norm=False, ax=axes[plot_idx])\n",
    "        axes[plot_idx].set_title(f'Distribution of {col} by {outcome}')\n",
    "        plot_idx += 1\n",
    "\n",
    "    # plot cat    \n",
    "    for col in cat_vars:\n",
    "        sns.countplot(data=df, x=col, hue=outcome, ax=axes[plot_idx])\n",
    "        axes[plot_idx].set_title(f'{col} counts by {outcome}')\n",
    "        axes[plot_idx].set_xticklabels([label.get_text()[:3] for label in axes[plot_idx].get_xticklabels()], rotation=45)\n",
    "        plot_idx += 1\n",
    "\n",
    "    # Remove unused axes\n",
    "    for i in range(plot_idx, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b5b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlations\n",
    "import seaborn as sns\n",
    "sns.pairplot(data.drop(columns=['nyu','res', 'home']), corner=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc785cf",
   "metadata": {},
   "source": [
    "#### Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1473ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function(data.drop(columns=['nyu','res']), 'home')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5991af",
   "metadata": {},
   "source": [
    "### Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca78aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function(data.drop(columns=['home','res']), 'nyu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82687ea",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f132bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function(data.drop(columns=['home','nyu']), 'res')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157175a",
   "metadata": {},
   "source": [
    "## Missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8105e12",
   "metadata": {},
   "source": [
    "Print missing data total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f3669",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.drop(columns=['nyu', 'res','home']).isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e5e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(data.drop(columns=['nyu', 'res', 'home']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db488e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.heatmap(data.drop(columns=['nyu', 'res', 'home']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4a4aa",
   "metadata": {},
   "source": [
    "Create an imputed dataset using median impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6cbb7393",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cols = ['pain', 'age', 'temperature', 'heartrate', 'resprate', 'o2sat', 'sbp', 'dbp', 'acuity']\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data_impute = data.copy()\n",
    "data_impute[m_cols] = imputer.fit_transform(data[m_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4503e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(data_impute.drop(columns=['nyu', 'res',  'home']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db78bf48",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e69ca",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ce82a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data\n",
    "def data_process(df_in, predictors, target_columns, drop_first_in=False, tab = 'not_tab'):\n",
    "\n",
    "   # Select relevant columns\n",
    "    data_predict = df_in[predictors + target_columns].copy()\n",
    "    \n",
    "    # Identify column types\n",
    "    numerical_cols = data_predict[predictors].select_dtypes(include='number').columns.tolist()\n",
    "    categorical_cols = data_predict[predictors].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    if tab == 'tab':\n",
    "        cat_idxs = []\n",
    "        cat_dims = []\n",
    "        for i, col in enumerate(categorical_cols):\n",
    "            le = LabelEncoder()\n",
    "            data_predict[col] = le.fit_transform(data_predict[col])\n",
    "            cat_idxs.append(data_predict.columns.get_loc(col))\n",
    "            cat_dims.append(data_predict[col].nunique())\n",
    "\n",
    "    # Train-test split\n",
    "    train_df, test_df = train_test_split(\n",
    "        data_predict, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=data_predict[target_columns]\n",
    "    )\n",
    "    \n",
    "    # Separate X and y\n",
    "    X_train = train_df[predictors].copy()\n",
    "    X_test = test_df[predictors].copy()\n",
    "    y_train = train_df[target_columns].copy()\n",
    "    y_test = test_df[target_columns].copy()\n",
    "\n",
    "    if tab == 'not_tab':\n",
    "        # One-hot encode categorical columns\n",
    "        X_train = pd.get_dummies(\n",
    "        X_train,\n",
    "        columns=categorical_cols,\n",
    "        drop_first=drop_first_in\n",
    "        )\n",
    "        X_test = pd.get_dummies(\n",
    "        X_test,\n",
    "        columns=categorical_cols,\n",
    "        drop_first=drop_first_in\n",
    "        )\n",
    "\n",
    "    # Align columns\n",
    "    X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "    # Sanity checks\n",
    "    print('Train X shape:', X_train.shape)\n",
    "    print(\"Test X shape:\", X_test.shape)\n",
    "\n",
    "    print(f\"\\nCheck proportions of outcome in data:\")\n",
    "    for col in y_train.columns:\n",
    "        print(f\"--- {col} ---\")\n",
    "        print(\"y_train proportions:\")\n",
    "        print(y_train[col].value_counts(normalize=True).sort_index())\n",
    "        print(\"y_test proportions:\")\n",
    "        print(y_test[col].value_counts(normalize=True).sort_index())\n",
    "\n",
    "    # Return fully separated objects\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "35a26ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost \n",
    "def xgb_f(X_train, y_train, X_test, y_test,rename_map, outcome, shap_in = True):\n",
    "    # Get original names for SHAP\n",
    "    original_names = X_train.columns.tolist()\n",
    "\n",
    "    # Calculate scale_pos_weight\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "    # Model\n",
    "    clf = XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='aucpr',\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight[outcome]\n",
    "    )\n",
    "    # Fit\n",
    "    model = clf.fit(X_train, y_train[outcome])\n",
    "    calibrator = CalibratedClassifierCV(model, method='isotonic', cv=5)\n",
    "    calibrator.fit(X_train, y_train[outcome].astype(int))\n",
    "    #Predict \n",
    "    y_pred = calibrator.predict(X_test)\n",
    "    y_proba = calibrator.predict_proba(X_test)[:, 1]\n",
    "    # Calculate AUC-PR with CI\n",
    "    n_bootstraps = 1000\n",
    "    rng = np.random.RandomState(42)  \n",
    "    bootstrapped_scores = []\n",
    "    y_test_array = np.array(y_test)\n",
    "    y_proba_array = np.array(y_proba)\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "        # Sample with replacement\n",
    "        indices = rng.choice(range(len(y_proba_array)), size=len(y_proba_array), replace=True)\n",
    "        score = average_precision_score(y_test_array[indices], y_proba_array[indices])\n",
    "        bootstrapped_scores.append(score)\n",
    "\n",
    "    sorted_scores = np.sort(bootstrapped_scores)\n",
    "    ci_lower = np.percentile(sorted_scores, 2.5)\n",
    "    ci_upper = np.percentile(sorted_scores, 97.5)\n",
    "    print(f\"PR-AUC: {average_precision_score(y_test_array, y_proba_array):.2f}\")\n",
    "    print(f\"95% CI: ({ci_lower:.2f}, {ci_upper:.2f})\")\n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f'AUC: {auc:.2f}')\n",
    "    # Print report\n",
    "    print(f\"{classification_report(y_test, y_pred, digits=2)}\\n\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\\n\")\n",
    "\n",
    "    # 4. Compute Spiegelhalter Z-statistic\n",
    "    E = np.sum(y_proba)  \n",
    "    O = np.sum(y_test.astype(int).values)  \n",
    "    V = np.sum(y_proba * (1 - y_proba))  \n",
    "    Z = (O - E) / np.sqrt(V)\n",
    "    print(f\"Spiegelhalter Z-statistic: {Z:.3f}\\n\")\n",
    "\n",
    "    # SHAP\n",
    "    if shap_in == False:\n",
    "        return\n",
    "    else:    \n",
    "        X_train_df = pd.DataFrame(X_train, columns=original_names)\n",
    "        X_train_renamed = X_train_df.rename(columns=rename_map)\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_train_renamed)\n",
    "\n",
    "        ## Table\n",
    "        feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "        importance_df = pd.Series(feature_importance, index=X_train_renamed.columns)\n",
    "        top_10_features = importance_df.sort_values(ascending=False).head(10)\n",
    "        print(\"Top 10 most important features by mean |SHAP| value:\")\n",
    "        print(top_10_features)\n",
    "    \n",
    "        ## Plot\n",
    "        shap.summary_plot(\n",
    "            shap_values, \n",
    "            X_train_renamed, \n",
    "            max_display=5,\n",
    "            show=False\n",
    "        )\n",
    "        fig = plt.gcf()\n",
    "        ax = plt.gca()\n",
    "        fig.text(\n",
    "            x=0.01,             \n",
    "            y=0.95,             \n",
    "            s=outcome,           \n",
    "            fontsize=14,\n",
    "            color='black',\n",
    "            ha='left',         \n",
    "            va='top'            \n",
    "        )\n",
    "        # Set font color to black\n",
    "        for text in ax.texts:\n",
    "            text.set_color('black')\n",
    "        # Save plot\n",
    "        fig.savefig(f\"{outcome}.svg\", format=\"svg\")\n",
    "        plt.show()\n",
    "\n",
    "# With HP tuning \n",
    "def xgb_tune(X_train, y_train, X_test, y_test, outcome):\n",
    "\n",
    "    # Calculate scale_pos_weight\n",
    "    target_series = y_train[outcome].astype(int)\n",
    "    scale_pos_weight = (target_series == 0).sum() / (target_series == 1).sum()\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 4, 5, 6],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1],\n",
    "        'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "        'gamma': [0, 1, 5],\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'scale_pos_weight': [scale_pos_weight * 0.5, scale_pos_weight, scale_pos_weight * 2],\n",
    "        'reg_alpha': [0,  0.1, 1, 10],     # L1 regularization\n",
    "        'reg_lambda': [0,  0.1, 1, 10]     # L2 regularization\n",
    "    }\n",
    "    \n",
    "    clf = XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='aucpr',\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    randomized_search = RandomizedSearchCV(\n",
    "        estimator=clf,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=50, \n",
    "        scoring='average_precision',\n",
    "        cv=cv,\n",
    "        n_jobs=1 \n",
    "    )\n",
    "\n",
    "    # Hyperparameter Fit\n",
    "    randomized_search.fit(X_train, y_train[outcome])\n",
    "    best_params = randomized_search.best_params_\n",
    "\n",
    "    # Train final model\n",
    "    final_model = XGBClassifier(\n",
    "        **best_params,\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='aucpr',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Fit final model\n",
    "    final_model.fit(X_train, y_train[outcome])\n",
    "    calibrator = CalibratedClassifierCV(final_model, method='isotonic', cv=5)\n",
    "\n",
    "    # Step 2: Fit on training data\n",
    "    calibrator.fit(X_train, y_train[outcome].astype(int))\n",
    "\n",
    "    # Step 3: Predict calibrated probabilities\n",
    "    y_pred = calibrator.predict(X_test)\n",
    "    y_proba = calibrator.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate AUC-PR\n",
    "    pr_auc = average_precision_score(y_test, y_proba)\n",
    "    print(f'PR AUC: {pr_auc:.2f}')\n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f'AUC: {auc:.2f}')\n",
    "    # Print report\n",
    "    print(classification_report(y_test, y_pred, digits=2))\n",
    "\n",
    "    # 4. Compute Spiegelhalter Z-statistic\n",
    "    E = np.sum(y_proba)  \n",
    "    O = np.sum(y_test.astype(int).values)  \n",
    "    V = np.sum(y_proba * (1 - y_proba))  \n",
    "    Z = (O - E) / np.sqrt(V)\n",
    "    print(f\"Spiegelhalter Z-statistic: {Z:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2a99f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet\n",
    "def elasticnet(X_train, y_train, X_test, y_test, outcome):\n",
    "    original_names = X_train.columns.tolist()\n",
    "\n",
    "    # scale numeric columns\n",
    "    num_cols = X_train.select_dtypes(include='number').columns.tolist()\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    X_train_scaled[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "    X_test_scaled[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "    # define and fit the model\n",
    "    model = LogisticRegressionCV(\n",
    "        cv = 5,\n",
    "        penalty='elasticnet',\n",
    "        l1_ratios=[0.1,0.5,0.9],\n",
    "        solver='saga',\n",
    "        max_iter=10000,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train[outcome])\n",
    "\n",
    "    # predict\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # evaluate\n",
    "    pr_auc = average_precision_score(y_test, y_proba)\n",
    "    print(f'PR AUC: {pr_auc:.2f}')\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f'AUC: {auc:.2f}')\n",
    "    print(classification_report(y_test, y_pred > 0.5, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a82f0b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabNet\n",
    "def tabnet(X_train, y_train, X_test, y_test, outcome):\n",
    "\n",
    "    # make into np arrays\n",
    "    X_train_np = X_train.to_numpy(dtype=np.float32)\n",
    "    X_test_np = X_test.to_numpy(dtype=np.float32)\n",
    "    y_train_np = y_train.to_numpy()\n",
    "    y_train_np = y_train_np.ravel()\n",
    "    y_test_np = y_test.to_numpy()\n",
    "    y_test_np = y_test_np.ravel()\n",
    "\n",
    "    # define the model\n",
    "    model = TabNetClassifier(\n",
    "        n_d=32,\n",
    "        n_a=32,\n",
    "        n_steps=3,\n",
    "        gamma=1.5,\n",
    "        n_independent=2,\n",
    "        n_shared=2,\n",
    "        momentum=0.3,\n",
    "        lambda_sparse=1e-4,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=0.001),\n",
    "        scheduler_params={\"step_size\": 10, \"gamma\": 0.9},\n",
    "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "        mask_type='entmax',\n",
    "        seed=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(\n",
    "        X_train=X_train_np, y_train=y_train_np,\n",
    "        eval_set=[(X_test_np, y_test_np)], \n",
    "        eval_name=['val'],\n",
    "        eval_metric=['auc'], \n",
    "        max_epochs=200,\n",
    "        patience=20,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # predict\n",
    "    y_pred = model.predict(X_test_np)\n",
    "\n",
    "    # probability for class 1\n",
    "    y_proba = model.predict_proba(X_test_np)[:, 1]\n",
    "\n",
    "    # calculate AUC-PR\n",
    "    pr_auc = average_precision_score(y_test, y_proba)\n",
    "    print(f'PR AUC: {pr_auc:.2f}')\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f'AUC: {auc:.2f}')\n",
    "\n",
    "    # print report\n",
    "    print(classification_report(y_test, y_pred > 0.5, digits=2))\n",
    "\n",
    "    return {\n",
    "        'model': model\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "21e6d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictors list\n",
    "predictors_in = [\n",
    "    'gender', \n",
    "    'age',\n",
    "    'race',\n",
    "    'arrival_transport',\n",
    "    'pain',\n",
    "    'temperature',\n",
    "    'heartrate',    \n",
    "    'resprate',\n",
    "    'o2sat',\n",
    "    'sbp',\n",
    "    'dbp',\n",
    "    'revisit_72hrs',\n",
    "    'med_gsn_count',\n",
    "    'acuity', \n",
    "    'cluster',\n",
    "    'CCI_cat',\n",
    "    'previous_cat', \n",
    "    'previous_result',\n",
    "    'time_of_day'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "adb99ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP variables list\n",
    "rename_map = {\n",
    "    'CCI_cat_0.0':'CCI:0*',\n",
    "    'CCI_cat_1.0':'CCI:1*',\n",
    "    'CCI_cat_2.0':'CCI:2*',\n",
    "    'CCI_cat_3.0':'CCI:3*',\n",
    "    'age': 'Age(years)',\n",
    "    'gender_M': 'Gender:Male*',\n",
    "    'gender_F': 'Gender:Female*',\n",
    "    'pain': 'Pain Score(/10)',\n",
    "    'race_WHITE': 'Race:White*',\n",
    "    'arrival_transport_ambulance': 'Arrival:Ambulance*',\n",
    "    'arrival_transport_walk in': 'Arrival:Walk In*',\n",
    "    'arrival_transport_unknown': 'Arrival:Unknown*',\n",
    "    'cluster_1': 'Complaint:Influenza like illness*',\n",
    "    'cluster_2': 'Complaint:Mental Status +\\nVehicle Collision*',\n",
    "    'cluster_3': 'Complaint:Chest pain +\\nDyspnea*',\n",
    "    'cluster_5': 'Complaint:Abdominal Pain*',\n",
    "    'cluster_7': 'Complaint:Wound eval\\ \\nAbnormal labs*',\n",
    "    'cluster_10': 'Complaint:Suicidal ideation*',\n",
    "    'cluster_11': 'Complaint:Fall*',\n",
    "    'dbp': 'Diastolic BP(mmhg)',\n",
    "    'sbp': 'Systolic BP(mmhg)',\n",
    "    'o2sat': 'Oxygen Saturation(%)',\n",
    "    'heartrate': 'Heart Rate(bpm)',\n",
    "    'resprate': 'Respiratory Rate(bpm)',\n",
    "    'temperature': 'Temperature(F)',\n",
    "    'med_gsn_count': 'Number of Medications(N)',\n",
    "    'acuity': 'Acuity(/5)',\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd90a077",
   "metadata": {},
   "source": [
    "## Home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c1b028",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef089f",
   "metadata": {},
   "source": [
    "#### Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d34884",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['home']\n",
    "\n",
    "# XGBoost and Elastic net need different one hot encoding strategies\n",
    "X_train_adxg, X_test_adxg, y_train_adxg, y_test_adxg = data_process(\n",
    "    data_impute, \n",
    "    predictors_in, \n",
    "    target_columns, \n",
    "    drop_first_in=False,\n",
    "    tab = 'not_tab'\n",
    ")\n",
    "\n",
    "X_train_ad, X_test_ad, y_train_ad, y_test_ad = data_process(\n",
    "    data_impute, \n",
    "    predictors_in, \n",
    "    target_columns, \n",
    "    drop_first_in=True,\n",
    "    tab = 'not_tab'\n",
    ")\n",
    "\n",
    "X_train_adtab, X_test_adtab, y_train_adtab, y_test_adtab = data_process(\n",
    "    data_impute, \n",
    "    predictors_in, \n",
    "    target_columns, \n",
    "    drop_first_in=False,\n",
    "    tab = 'tab'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee1a732",
   "metadata": {},
   "source": [
    "#### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_adraw, X_test_adraw, y_train_adraw, y_test_adraw = data_process(\n",
    "    data, \n",
    "    predictors_in, \n",
    "    target_columns, \n",
    "    drop_first_in=False,\n",
    "    tab = 'not_tab'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a76f13",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a302b74b",
   "metadata": {},
   "source": [
    "#### Imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d93df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_xgb = xgb_f(X_train_adxg,  y_train_adxg,X_test_adxg, y_test_adxg, rename_map, 'home', shap_in=True)\n",
    "\n",
    "with open('ad_xg.pkl', 'wb') as f:\n",
    "    pickle.dump(ad_xgb, f)\n",
    "\n",
    "del ad_xgb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b42fe0",
   "metadata": {},
   "source": [
    "##### With tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_xgb_tune = xgb_tune(X_train_adxg,  y_train_adxg,X_test_adxg, y_test_adxg,'home')\n",
    "\n",
    "with open('ad_xg_tune.pkl', 'wb') as f:\n",
    "    pickle.dump(ad_xgb_tune, f)\n",
    "\n",
    "del ad_xgb_tune\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f516137",
   "metadata": {},
   "source": [
    "##### With no acuity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1040f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_f(\n",
    "    X_train_adxg.drop('acuity', axis = 1),  \n",
    "    y_train_adxg,\n",
    "    X_test_adxg.drop('acuity', axis = 1), \n",
    "    y_test_adxg, rename_map, \n",
    "    'home', \n",
    "    shap_in = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681219dc",
   "metadata": {},
   "source": [
    "#### Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b09bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_raw = xgb_f(X_train_adraw,  y_train_adraw,X_test_adraw, y_test_adraw, rename_map, 'home', shap_in = False)\n",
    "\n",
    "with open('ad_raw.pkl', 'wb') as f:\n",
    "    pickle.dump(ad_raw, f)\n",
    "\n",
    "del ad_raw\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaa93f8",
   "metadata": {},
   "source": [
    "### Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08973382",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_el = elasticnet(X_train_ad, y_train_ad, X_test_ad, y_test_ad, 'home')\n",
    "\n",
    "with open('ad_el.pkl', 'wb') as f:\n",
    "    pickle.dump(ad_el, f)\n",
    "\n",
    "del ad_el\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca1c383",
   "metadata": {},
   "source": [
    "### Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85caf003",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_tabnet = tabnet(X_train_adtab, y_train_adtab, X_test_adtab, y_test_adtab, 'admitted')\n",
    "\n",
    "with open('ad_tabnet.pkl', 'wb') as f:\n",
    "    pickle.dump(ad_tabnet, f)\n",
    "\n",
    "del ad_tabnet\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0a34c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "admitted_data = {\n",
    "    'X_train_adxg': X_train_adxg,\n",
    "    'X_test_adxg': X_test_adxg,\n",
    "    'y_train_adxg': y_train_adxg,\n",
    "    'y_test_adxg': y_test_adxg,\n",
    "    'X_train_ad': X_train_ad,\n",
    "    'X_test_ad': X_test_ad,\n",
    "    'y_train_ad': y_train_ad,\n",
    "    'y_test_ad': y_test_ad,\n",
    "    'X_train_adtab': X_train_adtab,\n",
    "    'X_test_adtab': X_test_adtab,\n",
    "    'y_train_adtab': y_train_adtab,\n",
    "    'y_test_adtab': y_test_adtab,\n",
    "    'X_train_adraw': X_train_adraw,\n",
    "    'X_test_adraw': X_test_adraw,\n",
    "    'y_train_adraw': y_train_adraw,\n",
    "    'y_test_adraw': y_test_adraw\n",
    "}\n",
    "\n",
    "with open('admitted_data.pkl', 'wb') as f:\n",
    "    pickle.dump(admitted_data, f)\n",
    "    \n",
    "del X_train_adxg, X_test_adxg, y_train_adxg, y_test_adxg\n",
    "del X_train_ad, X_test_ad, y_train_ad, y_test_ad\n",
    "del X_train_adtab, X_test_adtab, y_train_adtab, y_test_adtab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93872d1e",
   "metadata": {},
   "source": [
    "## Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91407d34",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080cd38",
   "metadata": {},
   "source": [
    "#### Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c350737",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['nyu']\n",
    "\n",
    "# XGBoost and Elastic net need different one hot encoding strategies\n",
    "X_train_diagxg, X_test_diagxg, y_train_diagxg, y_test_diagxg = data_process(\n",
    "    data_impute, \n",
    "    predictors_in, \n",
    "    target_columns, \n",
    "    drop_first_in=False,\n",
    "    tab = 'not_tab'\n",
    ")\n",
    "\n",
    "X_train_diag, X_test_diag, y_train_diag, y_test_diag = data_process(\n",
    "    data_impute, \n",
    "    predictors_in, \n",
    "    target_columns, \n",
    "    drop_first_in=True,\n",
    "    tab = 'not_tab'\n",
    ")\n",
    "\n",
    "X_train_diagtab, X_test_diagtab, y_train_diagtab, y_test_diagtab = data_process(\n",
    "    data_impute, \n",
    "    predictors_in, \n",
    "    target_columns, \n",
    "    drop_first_in=False,\n",
    "    tab = 'tab'\n",
    ")\n",
    "\n",
    "X_train_diagac, X_test_diagac, y_train_diagac, y_test_diagac = data_process(\n",
    "    data_impute, \n",
    "    predictors_ac, \n",
    "    target_columns, \n",
    "    drop_first_in=True,\n",
    "    tab = 'tab'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['nyu']\n",
    "X_train_diagac, X_test_diagac, y_train_diagac, y_test_diagac = data_process(\n",
    "    data_impute, \n",
    "    predictors_ac, \n",
    "    target_columns, \n",
    "    drop_first_in=True,\n",
    "    tab = 'tab'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ecfa7c",
   "metadata": {},
   "source": [
    "#### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15882a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_diagraw, X_test_diagraw, y_train_diagraw, y_test_diagraw = data_process(\n",
    "    data, \n",
    "    predictors_in, \n",
    "    target_columns, \n",
    "    drop_first_in=False,\n",
    "    tab = 'not_tab'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109dc165",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6291d9b7",
   "metadata": {},
   "source": [
    "#### Imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyu_xgb = xgb_f(X_train_diagxg,  y_train_diagxg,X_test_diagxg, y_test_diagxg, rename_map, 'nyu', shap_in = True)\n",
    "\n",
    "with open('nyu_xgb.pkl', 'wb') as f:\n",
    "    pickle.dump(nyu_xgb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2383bc",
   "metadata": {},
   "source": [
    "##### With tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e04a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_xgb_tune = xgb_tune(X_train_diagxg,  y_train_diagxg,X_test_diagxg, y_test_diagxg,'nyu')\n",
    "\n",
    "with open('diag_xgb_tune.pkl', 'wb') as f:\n",
    "    pickle.dump(diag_xgb_tune, f)\n",
    "\n",
    "del diag_xgb_tune\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90bd009",
   "metadata": {},
   "source": [
    "##### With no acuity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_xgb_acuity = xgb_f(\n",
    "    X_train_diagxg.drop('acuity', axis = 1),  \n",
    "    y_train_diagxg,\n",
    "    X_test_diagxg.drop('acuity', axis = 1), \n",
    "    y_test_diagxg, \n",
    "    rename_map,\n",
    "    'nyu',\n",
    "    shap_in = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c174879",
   "metadata": {},
   "source": [
    "#### Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1133e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_raw = xgb_f(X_train_diagraw, y_train_diagraw, X_test_diagraw, y_test_diagraw, rename_map, 'nyu', shap_in = False)\n",
    "\n",
    "with open('diag_raw.pkl', 'wb') as f:\n",
    "    pickle.dump(diag_raw, f)\n",
    "\n",
    "del diag_raw\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3258f4ec",
   "metadata": {},
   "source": [
    "### Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c5f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyu_el = elasticnet(X_train_diag, y_train_diag, X_test_diag, y_test_diag, 'nyu')\n",
    "\n",
    "with open('nyu_el.pkl', 'wb') as f:\n",
    "    pickle.dump(nyu_el, f)\n",
    "\n",
    "del nyu_el\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fff80a",
   "metadata": {},
   "source": [
    "### Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a406e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyu_tabnet = tabnet(X_train_diagtab, y_train_diagtab, X_test_diagtab, y_test_diagtab, 'nyu')\n",
    "\n",
    "with open('nyu_tabnet.pkl', 'wb') as f:\n",
    "    pickle.dump(nyu_tabnet, f)\n",
    "\n",
    "del nyu_tabnet\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff62622",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyu_data = {\n",
    "    'X_train_diagxg': X_train_diagxg,\n",
    "    'X_test_diagxg': X_test_diagxg,\n",
    "    'y_train_diagxg': y_train_diagxg,\n",
    "    'y_test_diagxg': y_test_diagxg,\n",
    "    'X_train_diag': X_train_diag,\n",
    "    'X_test_diag': X_test_diag,\n",
    "    'y_train_diag': y_train_diag,\n",
    "    'y_test_diag': y_test_diag,\n",
    "    'X_train_diagtab': X_train_diagtab,\n",
    "    'X_test_diagtab': X_test_diagtab,\n",
    "    'y_train_diagtab': y_train_diagtab,\n",
    "    'y_test_diagtab': y_test_diagtab,\n",
    "    'X_train_diagraw': X_train_diagraw,\n",
    "    'X_test_diagraw': X_test_diagraw,\n",
    "    'y_train_diagraw': y_train_diagraw,\n",
    "    'y_test_diagraw': y_test_diagraw\n",
    "}\n",
    "\n",
    "with open('nyu_data.pkl', 'wb') as f:\n",
    "    pickle.dump(nyu_data, f)\n",
    "    \n",
    "del X_train_diagxg, X_test_diagxg, y_train_diagxg, y_test_diagxg\n",
    "del X_train_diag, X_test_diag, y_train_diag, y_test_diag\n",
    "del X_train_diagtab, X_test_diagtab, y_train_diagtab, y_test_diagtab\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b5b92",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4906a326",
   "metadata": {},
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af69ba73",
   "metadata": {},
   "source": [
    "#### Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ca10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['res']\n",
    "\n",
    "# XGBoost and Elastic net need different one hot encoding strategies\n",
    "X_train_resxg, X_test_resxg, y_train_resxg, y_test_resxg = data_process(\n",
    "    data_impute,\n",
    "    predictors_in, \n",
    "    target_columns, \n",
    "    drop_first_in=False,\n",
    "    tab = 'not_tab'\n",
    ")\n",
    "\n",
    "X_train_res, X_test_res, y_train_res, y_test_res = data_process(\n",
    "    data_impute, \n",
    "    predictors_in, \n",
    "    target_columns, \n",
    "    drop_first_in=True,\n",
    "    tab = 'not_tab'\n",
    ")\n",
    "\n",
    "X_train_restab, X_test_restab, y_train_restab, y_test_restab = data_process(\n",
    "    data_impute, \n",
    "    predictors_in, \n",
    "    target_columns, \n",
    "    drop_first_in=False,\n",
    "    tab = 'tab'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ccdd67",
   "metadata": {},
   "source": [
    "#### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e1c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resraw, X_test_resraw, y_train_resraw, y_test_resraw = data_process(\n",
    "    data, \n",
    "    predictors_in, \n",
    "    target_columns, \n",
    "    drop_first_in=False,\n",
    "    tab = 'not_tab'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5369d2",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bd26b2",
   "metadata": {},
   "source": [
    "#### Imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d4004",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_xgb = xgb_f(X_train_resxg, y_train_resxg, X_test_resxg, y_test_resxg, rename_map, 'res', shap_in=True)\n",
    "\n",
    "with open('res_xg.pkl', 'wb') as f:\n",
    "    pickle.dump(res_xgb, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720aeda7",
   "metadata": {},
   "source": [
    "##### With tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tune = xgb_tune(X_train_resxg, y_train_resxg, X_test_resxg, y_test_resxg, 'res')\n",
    "\n",
    "with open('res_tune.pkl', 'wb') as f:\n",
    "    pickle.dump(res_tune, f)\n",
    "\n",
    "del res_tune\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8667bc9",
   "metadata": {},
   "source": [
    "##### With no acuity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_f(\n",
    "    X_train_resxg.drop('acuity', axis = 1), \n",
    "    y_train_resxg, \n",
    "    X_test_resxg.drop('acuity', axis = 1), \n",
    "    y_test_resxg, \n",
    "    rename_map, \n",
    "    'res',\n",
    "    shap_in = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd0f28c",
   "metadata": {},
   "source": [
    "#### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a2b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_raw = xgb_f(X_train_resraw, y_train_resraw, X_test_resraw, y_test_resraw, rename_map, 'res', shap_in = False)\n",
    "\n",
    "with open('res_raw.pkl', 'wb') as f:\n",
    "    pickle.dump(res_raw, f)\n",
    "\n",
    "del res_raw\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f6554c",
   "metadata": {},
   "source": [
    "### Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_el = elasticnet(X_train_res, y_train_res, X_test_res, y_test_res, 'res')\n",
    "\n",
    "with open('res_el.pkl', 'wb') as f:\n",
    "    pickle.dump(res_el, f)\n",
    "\n",
    "del res_el\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e85bdd",
   "metadata": {},
   "source": [
    "### Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9722331",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tabnet = tabnet(X_train_res, y_train_res, X_test_res, y_test_res, 'res')\n",
    "\n",
    "with open('res_tabnet.pkl', 'wb') as f:\n",
    "    pickle.dump(res_tabnet, f)\n",
    "\n",
    "del res_tabnet\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d26d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_data = {\n",
    "    'X_train_resxg': X_train_resxg,\n",
    "    'X_test_resxg': X_test_resxg,\n",
    "    'y_train_resxg': y_train_resxg,\n",
    "    'y_test_resxg': y_test_resxg,\n",
    "    'X_train_res': X_train_res,\n",
    "    'X_test_res': X_test_res,\n",
    "    'y_train_res': y_train_res,\n",
    "    'y_test_res': y_test_res,\n",
    "    'X_train_restab': X_train_restab,\n",
    "    'X_test_restab': X_test_restab,\n",
    "    'y_train_restab': y_train_restab,\n",
    "    'y_test_restab': y_test_restab,\n",
    "    'X_train_resraw': X_train_resraw,\n",
    "    'X_test_resraw': X_test_resraw,\n",
    "    'y_train_resraw': y_train_resraw,\n",
    "    'y_test_resraw': y_test_resraw\n",
    "}\n",
    "\n",
    "\n",
    "with open('res_data.pkl', 'wb') as f:\n",
    "    pickle.dump(res_data, f)\n",
    "    \n",
    "del X_train_resxg, X_test_resxg, y_train_resxg, y_test_resxg\n",
    "del X_train_res, X_test_res, y_train_res, y_test_res\n",
    "del X_train_restab, X_test_restab, y_train_restab, y_test_restab\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic_data_sci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
